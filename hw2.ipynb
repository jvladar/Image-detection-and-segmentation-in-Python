{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d80532-7589-466b-8315-7243b788a264",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer Vision - Autumn 2021 - Home Assignment 2\n",
    "\n",
    "**Ondřej Schejbal & Jan Vladár**\n",
    "\n",
    "In our assigned we deal with problem of image detection and segmentation. We apply different approaches for template matching, descriptor detection and image segmentation of selected sea plants.\n",
    "\n",
    "For our project we decided to work with these 2 plant species:\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Zostera\">Zostera</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Zostera\">Mytilus</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19787c28-0338-46ec-be03-2407c0342404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class1FolderName = './Mytilus_original'\n",
    "class2FolderName = './Zostera_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2e6ce7e-3a0c-44f8-963a-b8e80b326c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the output images larger\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8372b775-9ef3-4d77-917a-fb2aa10961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFolderNameAndEtalonFolderName(classFolderName, print_it=False):\n",
    "    folderName = classFolderName[:-9]\n",
    "    etalonFolderName = folderName + 'Etalons'\n",
    "    if print_it:\n",
    "        print('Folder name:', folderName, '\\r\\nEtalon folder name:', etalonFolderName)\n",
    "    return folderName, etalonFolderName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0896e4-e6a6-403f-bf2c-4a45a1eda3c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1\n",
    "*Collect a set of images suitable for the tasks below of at least 2 species. Write code to preprocess the \n",
    "images of plants into a uniform size of your choice, e.g. 1024x1024 pixels.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddf4cd",
   "metadata": {},
   "source": [
    "We have searched for relevant **Zostera** and **Mytilus** images on the web, but most of the pictures contained either some form of watermark, or some additional image distortion. Because of that we were left with only limited number of images for our task. This most likely had significant impact, especially on the deep neural network solution described in [part 4](#part_4) and that's why we decided to use data agmentation for that part of our assignment.\n",
    "\n",
    "In the code below we are transforming the images into recommended dimensions 1024x1024 while also converting them to the *.png* format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed9029bc-087d-4975-b540-d8f85ee19482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformImagesInDirectory(folderName):\n",
    "    list_of_files = os.listdir(folderName)\n",
    "    targetFolderName = folderName[:-9]\n",
    "    for idx, file in enumerate(list_of_files):\n",
    "        image_file_name = os.path.join(folderName, file)\n",
    "        img = Image.open(image_file_name)  # .convert(\"L\")\n",
    "        img = img.resize((1024, 1024))\n",
    "        if not os.path.exists(targetFolderName):\n",
    "            os.mkdir(targetFolderName)\n",
    "        img.save(targetFolderName + '/' + str(idx) + \".png\")\n",
    "        # os.remove(image_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec1af1a-5d0b-4820-9e4f-34a9fc67682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1():\n",
    "    transformImagesInDirectory(class1FolderName)\n",
    "    transformImagesInDirectory(class2FolderName)\n",
    "# task1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545741c-f4dc-4ec5-9d04-38a6a9babf67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2<a id='part_2'></a>\n",
    "*Select a set of etalons (e.g. small images containing a sample of some distinctive features) from the \n",
    "an image to be used for matching similar objects. Aim for a set that would be representative on at \n",
    "least 50% of the images of the particular species. Think how to deal with rotations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e74f6",
   "metadata": {},
   "source": [
    "In this task we have manually created 9 etalons for each selected sea plant. We have selected the etalons as small as possible, but still containing representative and dominant part of given plants visual.\n",
    "\n",
    "For Mytilus this was quite simple, but for Zostera it was not. Zostera is basically a sea grass and selecting only one grass stalk was sometimes very challenging. We have also decided to create Zostera etalons from different parts of the grass stalks - from the top, middle and bottom of the stalk.\n",
    "\n",
    "When selecting the etalons our goal was to have a selection of small pieces from our images, which would have enough representative characteristic needed for succesfull template matching with at least 50 % accuracy for the respective plant species.\n",
    "\n",
    "In the cells below you can see us matching the created etalons one by one to each image in our dataset. We have used the opencv2 <a href=\"https://docs.opencv.org/4.x/df/dfb/group__imgproc__object.html#ga586ebfb0a7fb604b35a23d85391329be\">matchTemplate</a> function. This function has many matching methods and after some experiments and studying the differences between them we have decided to use the **TM_CCOEFF_NORMED** match method for this task, which represents the normalized value of correlation coefficient between the images.\n",
    "\n",
    "<p style=\"display:none\">https://stackoverflow.com/questions/55469431/what-does-the-tm-ccorr-and-tm-ccoeff-in-opencv-mean</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b824b",
   "metadata": {},
   "source": [
    "**Resolution**\n",
    "\n",
    "We were able to achieve ~60 % of accuracy for Zostera and ~55 % accuracy for Mytilus.\n",
    "\n",
    "We have also experimented with etalons. We observed that modifying the size significantly affects the accuracy. When the etalons were much bigger, the accuracy became significantly lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23c87ccb-d97b-41e6-8d58-3d51717f94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_image(img, template):\n",
    "    w, h = template.shape[::-1]\n",
    "    match_method = cv2.TM_CCOEFF_NORMED\n",
    "    res = cv2.matchTemplate(img, template, match_method)\n",
    "    _, maxval, _, maxloc = cv2.minMaxLoc(res)\n",
    "    btm_right = (maxloc[0] + w, maxloc[1] + h)\n",
    "    cv2.rectangle(img, maxloc, btm_right, 255, 2)\n",
    "    return maxval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c56360a9-ae76-4863-a110-bf483163508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchEtalonToEachImage(etalons, images):\n",
    "    # matching_results = []\n",
    "    matching_results_by_etalon = []\n",
    "    for etalon_name in os.listdir(etalons):\n",
    "        etalon_path = etalons + \"/\" + etalon_name\n",
    "        # print(\"etalon:\", etalon_path)\n",
    "        img_template = cv2.imread(etalon_path, 0) \n",
    "        matching_results = []\n",
    "        for image_name in os.listdir(images):\n",
    "            img_path = images + \"/\" + image_name\n",
    "            # print(\"\\t img\", img_path)\n",
    "            img = cv2.imread(img_path, 0)\n",
    "            match_res = match_image(img, img_template)\n",
    "            matching_results.append(match_res)\n",
    "        matching_results_by_etalon.append(matching_results)\n",
    "\n",
    "    averages = []\n",
    "\n",
    "    for val in matching_results_by_etalon:\n",
    "        averages.append(np.average(val))\n",
    "    print(\"Precision:\", np.average(averages) * 100)\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3e28264-0ddd-4849-a64e-b3e03a72889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mytilus\n",
      "Precision: 54.667162322081055\n",
      "Zostera\n",
      "Precision: 59.799787082842414\n"
     ]
    }
   ],
   "source": [
    "def task2(classFolderName):\n",
    "    folderName, etalonFolderName = getFolderNameAndEtalonFolderName(classFolderName)\n",
    "    precisionForEachEtalonList = matchEtalonToEachImage(etalonFolderName, folderName)\n",
    "    # print(\"Precisions for each etalon:\", precisionForEachEtalonList)\n",
    "\n",
    "print(\"Mytilus\")\n",
    "task2(class1FolderName)\n",
    "\n",
    "print(\"Zostera\")\n",
    "task2(class2FolderName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544597b-1735-4336-8095-10fc3638575e",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Use at least 3 different existing conventional feature detectors provided by \n",
    "OpenCV to find matches of the etalons in the image. NB! Take into account overlaps and subtract the \n",
    "appropriate numbers from total scores.\n",
    "\n",
    "Evaluate on two different images (called task3a.tiff and task3b.tiff) how well the approach works and \n",
    "which feature detector performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370db196",
   "metadata": {},
   "source": [
    "For this task we were supposed to use 3 feature detectors to find matches of the etalons in the image. We have decided to use 3 different approaches:\n",
    "1. SIFT with FlannBased matching\n",
    "2. ORB with BruteForce matching\n",
    "3. SIFT with BruteForce matching\n",
    "\n",
    "Our implementation is in the *hw2_3.py* file.\n",
    "\n",
    "Our results can be seen in the *Task3Results* file. Here we can observe the difference between selected feature detectors.\n",
    "\n",
    "We have observed that ORB does not produce as good results as SIFT. It matches some features correctly but it's mostly able to match only few descriptors correctly.\n",
    "\n",
    "When using the SIFT feature detector we have decided that we will only consider descriptors that are close to each other. With this we wanted to prevent matching points which are out of our range of interest. For this we have used the <a href=\"https://stackoverflow.com/questions/51197091/how-does-the-lowes-ratio-test-work\">Lowe's ratio test</a>.\n",
    "\n",
    "**BruteForce vs. FlannBased matcher comparison**\n",
    "\n",
    "BruteForce matcher is going to try all the possibilities (which is the meaning of \"Brute Force\") and hence it will find the best matches.\n",
    "FLANN, meaning \"Fast Library for Approximate Nearest Neighbors\", will be much faster but will find an approximate nearest neighbors. \n",
    "It will find a good matching, but not necessarily the best possible one. We have experimented with the FLANN's parameters in order to increase the precision (\"quality\" of the matchings), and we have observed that in some cases it was negatively affecting the speed of the algorithm.\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "When comparing the ORB and both SIFT detectors we were able to see that the SIFT detectors perform much better. When we compared the SIFT matching techniques we have come to conclusion that FLANN is much faster than BFMatcher but it only finds an approximate nearest neighbor, which is a good matching but not necessarily the best. Anyway none of the selected detectors performed well when matching etalons with an image which was not containing the etalon. Our results on selected 2 images can be seen in the *Task3Results* for each 3 feature detectors.\n",
    "\n",
    "We have also observed that if we rotate the etalon, the approach used in [task 2](#part_2) is not able to find the etalon in the image. But here, using the methods mentioned above we are able to find the etalons even if they were rotated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab3b22",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"Task3Results/Orb0.png\" width=30%/>\n",
    "    <img src=\"Task3Results/SiftBruteForce.png\" width=30%/>\n",
    "    <img src=\"Task3Results/SiftFlann.png\" width=30%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a541c-9052-4fdc-baca-1ce5da61cc5e",
   "metadata": {},
   "source": [
    "## Task 4<a id='part_4'></a>\n",
    "*Improve the baseline by applying deep learning.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a0bfb7",
   "metadata": {},
   "source": [
    "Our model implementation can be found in the root of this project's directory and consists of these files:\n",
    "* dataset.py\n",
    "* mask_creator.py\n",
    "* model.py\n",
    "* train.py\n",
    "* utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbcea7",
   "metadata": {},
   "source": [
    "First we have annotated our data using the <a href=\"https://cvat.org/\">CVAT</a> annotation tool. We have distinguished 3 classes: Zostera, Mytilus and the background. We have then exported the annotations in the COCO format.\n",
    "\n",
    "Then we have used the <a href=\"\">**COCO**</a> (Common Objects in Context) library to load the exported annotations and we have prepared relevant mask and frame (original image with highlighted mask) images using it's build in functions.\n",
    "\n",
    "Masks and Frames are saved and can be seen in folders with the same name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbdda9",
   "metadata": {},
   "source": [
    "At first we have decided to approach the problem using prebuild models in TensorFlow. We prepared data generator functions and also applied the concept of data augmentation to prepare augmented data generator. Then we have tried to run the MobileNetV2 network on our prepared data, but at this point we have occured a problem with our data. We have tried a lot of different modification, but in each case we were either getting bad results from the model or not providing the model with correct data format.\n",
    "\n",
    "Our implementation can be found in the *hw2_4.py*.\n",
    "\n",
    "Because this approach was not successfull we have decided to try using the *pytorch* library instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f396617",
   "metadata": {},
   "source": [
    "After some research we have found an interesting article which presented cropped version of the UNET model's architecture. Paper with the models architecture can be seen in **U-Net_Convolutional_Networks.pdf**.\n",
    "\n",
    "We have used our masks created in the previous steps and prepared *train.py* script which allowed us to easily modifythe model's parameters. We have experimented with different values of the epochs count, batch size and different resizing of the input images in order to get the best segmentation accuracy. We have decided to split the dataset into train and validation sets with the ration 5:3.\n",
    "\n",
    "Our best result was achieved after 56 epochs with accuracy around 40 %.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856beee0",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "In the end we were able to successfully prepare working model for our plant segmentation task. The accuracy of the model was not really good, but that was expected given the low number of images in our dataset.\n",
    "\n",
    "Since in task 3 we have used feature detection and in task 4 we were performing image segmentation there is not a clear way how to compare these two approaches. We have observed that the feature detection is not working very well, since it finds only exact matches of our etalons. The neural network provides much better results since it is able to detect the plants on it's own without the need of etalons. On the other hand it's need good data annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396314c",
   "metadata": {},
   "source": [
    "### Example of our original image, it's mask and the prediction.\n",
    "In the prediction the more dark parts of the image signalizes that the model was mor sure with it's prediction.\n",
    "\n",
    "<div>\n",
    "    <img src=\"Mytilus/musla10.png\" width=30%/>\n",
    "    <img src=\"result_images/1.png\" width=30%/>\n",
    "    <img src=\"result_images/pred_1.png\" width=30%/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
